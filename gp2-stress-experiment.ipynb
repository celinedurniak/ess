{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to start\n",
    "\n",
    "Before starting you must:\n",
    "- Ensure that `scipp` and `mantid` are on your `PYTHONPATH`.\n",
    "- Generate the `config.py` file using `make_config.py`. Refer to the `README.md` or `python make_config.py --help` for information.\n",
    "- Install dependencies : `conda install fabio tifffile` (used for image handling)\n",
    "\n",
    "For `scipp` and `mantid` follow instructions at: https://scipp.readthedocs.io/en/latest/getting-started/installation.html.\n",
    "\n",
    "Converted to use scipp and notebook from [this repistory](https://git.esss.dk/testbeamline/gp2/-/tree/stressexperiment).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    import scipp\n",
    "except ImportError as e:\n",
    "    print(\"scipp is not available in the PYTHONPATH\")\n",
    "    raise e\n",
    "    \n",
    "try:\n",
    "    import mantid\n",
    "except ImportError as e:\n",
    "    print(\"mantid is not available in the PYTHONPATH\")\n",
    "    raise e\n",
    "    \n",
    "try:\n",
    "    import scippconfig\n",
    "except ImportError as e:\n",
    "    print(\"scippconfig is not available. Make sure you have generated it with `make_config.py`.\")\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets get everything set up\n",
    "from multiprocessing.dummy import Pool as ThreadPool\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import scipp as sc\n",
    "import numpy as np\n",
    "\n",
    "import imaging, operations\n",
    "\n",
    "from scipy import ndimage, signal\n",
    "\n",
    "DATA_DIR_NAME = \"data_GP2\"\n",
    "OUTPUT_DIR_NAME = \"output\"\n",
    "\n",
    "data_dir = os.path.join(scippconfig.script_root, DATA_DIR_NAME)\n",
    "output_dir = os.path.join(scippconfig.script_root, OUTPUT_DIR_NAME)\n",
    "instrument_file = os.path.join(scippconfig.script_root, 'IDF', 'V20_Definition_GP2.xml')\n",
    "\n",
    "if not os.path.exists(data_dir):\n",
    "    raise FileNotFoundError(\"The following data directory does not exist,\"\n",
    "                            f\" check your make_config.py:\\n{data_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Customisable Options:\n",
    "\n",
    "# Whether or not to do the plotting.\n",
    "do_plots = True\n",
    "\n",
    "# defining grouping of 2D detector pixels\n",
    "grouping_number = 27\n",
    "nx_target = grouping_number\n",
    "ny_target = grouping_number\n",
    "\n",
    "# Rebin regions for each of the 5 frames\n",
    "# in the format of [bin-start, bin-end, bin width].\n",
    "# used to crop each image, before stitching them together\n",
    "frame_parameters = [(15450, 22942, 64),\n",
    "                    (24800, 32052, 64),\n",
    "                    (33791, 40084, 64),\n",
    "                    (41763, 47457, 64),\n",
    "                    (49315, 54500, 64),\n",
    "                    (56500, 58360, 64)]\n",
    "\n",
    "# Used to shift the cropped frames so that their bins overlap \n",
    "# before summing them together into a single frame\n",
    "frame_shift_increments = [-6630, -2420, -2253, -2095, -1946, -1810]\n",
    "frame_shift_increments = [float(i) for i in frame_shift_increments]  # Work around #1114\n",
    "\n",
    "# Used to rebin the summed frame in order to\n",
    "# cut off frames that contain no data\n",
    "rebin_parameters = {\"start\": 8550, \"stop\": 26000, \"width\": (64 * 2.5)}\n",
    "\n",
    "# Pulse references\n",
    "pulse_number_reference = 1.0 / 770956\n",
    "pulse_number_sample = 1.0 / 1280381\n",
    "pulse_number_sample_elastic = 1.0 / 2416839\n",
    "pulse_number_sample_plastic = 1.0 / 2614343\n",
    "\n",
    "# units of transmission, all pixels with transmission higher masking threshold are masked\n",
    "masking_threshold = 0.80\n",
    "\n",
    "# Toggles outputting masked and sliced tiff stacks\n",
    "output_tiff_stack = True\n",
    "\n",
    "# Experiment Metadata\n",
    "measurement_number = 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "frame_shifts = [sum(frame_shift_increments[:i + 1]) for i in range(len(frame_shift_increments))]\n",
    "\n",
    "# let's get the process started:\n",
    "tofs_path = os.path.join(data_dir, 'metadata', 'GP2_Stress_time_values.txt')\n",
    "raw_data_dir = os.path.join(data_dir, \"Stress_Experiments\")\n",
    "\n",
    "ds = sc.Dataset()\n",
    "# Load X values from the TOF file\n",
    "ds.coords[\"tof\"] = sc.Variable([\"tof\"], unit=sc.units.us, values=imaging.read_x_values(tofs_path))\n",
    "ds.coords[\"tof\"] *= 1e3\n",
    "\n",
    "def load_and_scale(folder_name, scale_factor):\n",
    "    to_load = os.path.join(raw_data_dir, folder_name)\n",
    "    variable = imaging.tiffs_to_variable(to_load)\n",
    "    variable *= scale_factor\n",
    "    return variable\n",
    "\n",
    "ds[\"reference\"] = load_and_scale(folder_name=\"1) R825 Open Beam\", scale_factor=pulse_number_reference)\n",
    "ds[\"sample\"] = load_and_scale(folder_name=\"2) R825\", scale_factor=pulse_number_sample)\n",
    "ds[\"sample_elastic\"] = load_and_scale(folder_name=\"3) R825 600 Mpa\", scale_factor=pulse_number_sample_elastic)\n",
    "ds[\"sample_plastic\"] = load_and_scale(folder_name=\"4) R825 3500 um\", scale_factor=pulse_number_sample_plastic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Adds a coordinate for the spectra and TOF\n",
    "ds.coords[\"spectrum\"] = sc.Variable([\"spectrum\"], values=np.arange(ds[\"sample\"].shape[1]))\n",
    "stitched = sc.Dataset(coords={\"tof\": sc.Variable([\"tof\"], unit=sc.units.us,\n",
    "                              values=np.arange(start=rebin_parameters[\"start\"], stop=rebin_parameters[\"stop\"],\n",
    "                                               step=rebin_parameters[\"width\"], dtype=np.float64))})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def stitch_data(variable_to_stitch):\n",
    "    print(f\"Stitching: {variable_to_stitch}\")\n",
    "    stitched[variable_to_stitch] = imaging.stitch(ds[variable_to_stitch],\n",
    "                                                  frame_parameters=frame_parameters,\n",
    "                                                  frame_shifts=frame_shifts,\n",
    "                                                  rebin_parameters=rebin_parameters)\n",
    "\n",
    "for var_name in ds.keys():\n",
    "    stitch_data(var_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tof = stitched.coords['tof']\n",
    "bin_widths = tof['tof', 1:] - tof['tof', :-1]\n",
    "\n",
    "integrated = sc.Dataset()\n",
    "for dim in stitched.coords.keys():\n",
    "    integrated.coords[dim] = stitched.coords[dim]\n",
    "\n",
    "integrated = sc.sum(stitched, 'tof')\n",
    "# Put TOF coords back for the export to TIFF stack later\n",
    "integrated.coords[\"tof\"] = stitched.coords[\"tof\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "num_spectra = stitched.coords[\"spectrum\"].shape[0]\n",
    "bank_width = int(np.sqrt(num_spectra))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Pull out reference from out dataset to avoid checking for it in loops\n",
    "reference = stitched[\"reference\"]\n",
    "integrated_reference = integrated[\"reference\"]\n",
    "\n",
    "# Causes segmentation fault, see scipp/scipp issue: #1176\n",
    "# del stitched[\"reference\"]\n",
    "# del integrated[\"reference\"]\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def mask_non_sample_regions(integrated_dataset, var_name):\n",
    "    integrated_spectra = integrated_dataset[var_name]\n",
    "\n",
    "    # This should use sc.greater once #1178 is completed\n",
    "    spectra_masks = np.greater(integrated_spectra.values, masking_threshold)\n",
    "    spectra_masks = sc.Variable([\"spectrum\"], values=spectra_masks)\n",
    "    final_mask = operations.mask_from_adj_pixels(spectra_masks, bank_width)\n",
    "    stitched.masks[var_name] = final_mask\n",
    "\n",
    "\n",
    "integrated /= integrated_reference\n",
    "\n",
    "for k in integrated.keys():\n",
    "    if k == \"reference\":  # This can be removed once #1176 is resolved\n",
    "        continue\n",
    "\n",
    "    print(f\"Masking non-sample regions in {k}\")\n",
    "    mask_non_sample_regions(integrated, k)\n",
    "\n",
    "    if output_tiff_stack:\n",
    "        print(f\"Exporting tiff stack for {k}\")\n",
    "        tiff_out_dir = os.path.join(output_dir, f\"tiffs_tof_sum_{k}\")\n",
    "        imaging.export_tiff_stack(dataset=integrated, key=k,\n",
    "                                  base_name=f\"{k}_norm_sum\",\n",
    "                                  output_dir=tiff_out_dir,\n",
    "                                  x_len=324, y_len=324)\n",
    "        # Print a newline to prevent the saving messages overlapping\n",
    "        print()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Normalize by open beam\n",
    "normalized = stitched / reference\n",
    "\n",
    "# Replace special values nan and inf\n",
    "replacement=sc.Variable(value=0.0, variance=0.0)\n",
    "kwargs = {\"nan\" : replacement, \"posinf\" : replacement, \"neginf\" : replacement}\n",
    "for k in normalized.keys():\n",
    "    sc.nan_to_num(normalized[k].data, out=normalized[k].data, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if output_tiff_stack:\n",
    "    imaging.export_tiff_stack(dataset=normalized, key=\"sample\", x_len=324, y_len=324,\n",
    "                              base_name=\"initial_tof\", output_dir=os.path.join(output_dir, \"tiffs_tof_initial\"))\n",
    "    imaging.export_tiff_stack(dataset=normalized, key=\"sample_elastic\", x_len=324, y_len=324,\n",
    "                              base_name=\"elastic_tof\", output_dir=os.path.join(output_dir, \"tiffs_tof_elastic\"))\n",
    "    imaging.export_tiff_stack(dataset=normalized, key=\"sample_plastic\", x_len=324, y_len=324,\n",
    "                          base_name=\"elastic_tof\", output_dir=os.path.join(output_dir, \"tiffs_tof_plastic\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sc.compat.mantid.load_component_info(normalized, instrument_file)\n",
    "wavelength = sc.neutron.convert(normalized, \"tof\", \"wavelength\")\n",
    "\n",
    "# Position data ...etc. is dropped on conversion but is later required for plotting\n",
    "sc.compat.mantid.load_component_info(wavelength, instrument_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Apply a median filter to the sample after converting to wavelength\n",
    "apply_median_filter = True\n",
    "\n",
    "def median_filter(dataset, variable_key):\n",
    "    print(f\"Running median of workspace {variable_key}\")\n",
    "    data = dataset[variable_key].values\n",
    "    masks = dataset.masks[variable_key].values\n",
    "\n",
    "    # Repeat does not allocate mem, and makes it easier to double check\n",
    "    # masking is being handled properly using debugging tooling\n",
    "    mask = np.repeat(masks[np.newaxis, :], data.shape[0], axis=0)\n",
    "\n",
    "    masked_data = np.ma.masked_array(data, mask=mask)\n",
    "\n",
    "    # The number\n",
    "    result = ndimage.median_filter(masked_data,\n",
    "                                   size=(data.shape[0], 3),  # Take all wavelength vals for 2 adj spectra\n",
    "                                   mode=\"constant\", cval=0.)  # Set any vals outside median window to 0.\n",
    "    dataset[variable_key].values = result\n",
    "\n",
    "if apply_median_filter:\n",
    "    for k in wavelength.keys():\n",
    "        if k == \"reference\":  # This can be removed once #1176 is resolved\n",
    "            continue\n",
    "\n",
    "        new_result = median_filter(dataset=wavelength, variable_key=k)\n",
    "\n",
    "    # TODO export_to_tiff_stack for each"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "wav_x_vals = wavelength[\"sample\"].coords[\"wavelength\"].values\n",
    "start_wave = np.max(wav_x_vals[:, 0])\n",
    "stop_wave = np.min(wav_x_vals[:, -1])\n",
    "bin_width_wave = np.mean(wav_x_vals[:, 1:]-wav_x_vals[:, :-1])\n",
    "wavelength_rebin_params = sc.Variable([\"wavelength\"], values=np.arange(start_wave, stop_wave, bin_width_wave,\n",
    "                                                           dtype=np.float64))\n",
    "\n",
    "rebinned_wavelength = sc.rebin(wavelength, \"wavelength\", wavelength_rebin_params)\n",
    "\n",
    "# if True, the whole initial state is integrated, if False initial state is also grouped\n",
    "integrating_of_sample_in_inital_state = True\n",
    "if integrating_of_sample_in_inital_state:\n",
    "    group_size = (324.0  / nx_target) * (324.0 / ny_target)\n",
    "    factor = group_size\n",
    "    rebinned_wavelength[\"sample\"] *= factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def weighted_sum(variable : sc.Variable, dim_to_sum):\n",
    "    variable = variable.copy()\n",
    "\n",
    "    variances = sc.Variable(dims=variable.dims, values=variable.variances)\n",
    "    d = sc.Dataset()\n",
    "    d['data'] = variable / variances\n",
    "    d['norm'] = sc.reciprocal(variances)\n",
    "\n",
    "    d.masks['zero_error'] = sc.equal(variances, 0.0 * variances.unit)\n",
    "    \n",
    "    d = sc.sum(d, dim_to_sum)\n",
    "    sum = d['data']/d['norm']\n",
    "    return sum\n",
    "\n",
    "# If we are not interested in strain regions for the unloaded sample, we can just combine all spectra to improve statistics.\n",
    "# maybe another masking before summation, with tougher threshold to exclude boderline pixels.\n",
    "rebinned_wavelength[\"sample\"] = weighted_sum(rebinned_wavelength[\"sample\"], \"spectrum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "rebinned_wavelength.coords[\"detector_mapping\"] = imaging.make_detector_groups(324, 324, nx_target, ny_target)\n",
    "grouped = sc.groupby(rebinned_wavelength, \"detector_mapping\").sum(\"spectrum\")\n",
    "grouped_sample_elastic = grouped[\"sample_elastic\"]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "histogram_matrices = [rebinned_wavelength[\"sample\"], grouped_sample_elastic]\n",
    "template_histogram = histogram_matrices[1]\n",
    "\n",
    "# Average out detector positions\n",
    "positions_dataset = sc.Dataset()\n",
    "positions_dataset[\"position\"] = sc.Variable(rebinned_wavelength.coords[\"position\"])\n",
    "positions_dataset.coords[\"detector_mapping\"] = rebinned_wavelength.coords[\"detector_mapping\"]\n",
    "grouped_positions = sc.groupby(positions_dataset, \"detector_mapping\").mean(\"spectrum\")\n",
    "\n",
    "plots = []\n",
    "for spec_num in template_histogram.coords[\"detector_mapping\"].values:\n",
    "    pos = grouped_positions[\"position\"][\"detector_mapping\", int(spec_num)]\n",
    "    # Append spectrum index and position\n",
    "    plots.append((spec_num,pos))\n",
    "\n",
    "\n",
    "# We cant access position using (dim, I), since the vector 3d has not dim currently\n",
    "Y_key = 1\n",
    "X_key = 0\n",
    "\n",
    "# We must use round to prevent floating points errors during sorting like -0.05 > 0.05\n",
    "plots.sort(key=lambda t: (round(-t[1].value[Y_key], 10), round(t[1].value[X_key], 10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Bragg edge position roughly, in Angstroem\n",
    "FCC_a = 3.5 # Aangstroem, taken from COD entry 9008469\n",
    "BCC_a = 2.865 # Aangstroem, average value from COD\n",
    "indices_FCC = [(1,1,1),(2,0,0),(2,2,0),(3,1,1)]#,(2,2,2)]#,(4,0,0)]\n",
    "indices_BCC = [(1,1,0),(2,0,0),(2,1,1),(2,2,0)]#,(3,1,0),(2,2,2)]\n",
    "\n",
    "def create_Braggedge_list(lattice_constant, Miller_indices):\n",
    "    '''\n",
    "    :param Miller_indices: like [(1,1,0),(2,0,0),...]\n",
    "    :type Miller_indices: list of tuples\n",
    "    '''\n",
    "    from numpy import sqrt\n",
    "    return [(d ,2.*lattice_constant/sqrt(d[0]**2+d[1]**2+d[2]**2)) for d in Miller_indices]\n",
    "\n",
    "\n",
    "Bragg_edges_FCC = create_Braggedge_list(FCC_a, indices_FCC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def convert_var_to_ws(variable, dim):\n",
    "    x_vals = np.array(variable.coords[\"wavelength\"].values)\n",
    "\n",
    "    ws = sc.compat.mantid.to_workspace_2d(x=x_vals,\n",
    "                                          y=np.transpose(variable.values),\n",
    "                                          e=np.transpose(variable.variances),\n",
    "                                          coord_dim=dim,\n",
    "                                          instrument_file=instrument_file)\n",
    "    return ws\n",
    "\n",
    "\n",
    "def Bragg_edge_position(xpos_Bragg_edge,x_min_sides,x_max_sides,x_size,y_size,plots):\n",
    "    '''Takes limits around a roughly known Bragg edge and fits the Bragg edge to obtain\n",
    "    its exact position. Needs the size (x_size, y_size) and the workspaces containing histogram arrays/matrices.\n",
    "    x_min_side and x_max_side are arbitrary values and are calculated:\n",
    "    bragg_edge_pos * x_min_side or bragg_edge_pos * x_max_side.\n",
    "    '''\n",
    "\n",
    "    fit_list = []\n",
    "    spectrum_list_fitted_sample = []\n",
    "    spectrum_list_fitted_sample_elastic = []\n",
    "\n",
    "    if not (len(xpos_Bragg_edge) == len(x_min_sides) or len(xpos_Bragg_edge) == len(x_max_sides)):\n",
    "        print(\"xpos_Bragg_edge has not the same length as either x_min_sides or x_max_sides. Check your input.\")\n",
    "        return\n",
    "    \n",
    "    elastic_ws = convert_var_to_ws(grouped_sample_elastic, \"wavelength\")\n",
    "    sample_ws = convert_var_to_ws(rebinned_wavelength[\"sample\"], \"wavelength\")\n",
    "\n",
    "    # loop for each Bragg edge in the list, find tof positions and save spectrums in lists\n",
    "    for c, values in enumerate(xpos_Bragg_edge):\n",
    "        fit_list.append([])\n",
    "        spectrum_list_fitted_sample.append([])\n",
    "        spectrum_list_fitted_sample_elastic.append([])\n",
    "        bragg_edge = '({}{}{})'.format(values[0][0], values[0][1], values[0][2])\n",
    "        xpos_guess = values[1]\n",
    "        x_min_fit = xpos_guess - xpos_guess * abs(x_min_sides[c])\n",
    "        x_max_fit = xpos_guess + xpos_guess * abs(x_max_sides[c])\n",
    "\n",
    "        print(\"Now fitting Bragg edge {} at {:.3f} A (between {:.3f} A and {:.3f} A)\".format(bragg_edge, xpos_guess, x_min_fit, x_max_fit))\n",
    "\n",
    "\n",
    "        # if the full inital sample was taken and no grouping was done\n",
    "        if \"spectrum\" not in rebinned_wavelength[\"sample\"].coords:\n",
    "            print('Sample workspace contains one histogram.')\n",
    "            print(f'Elastic Workspace contains {wavelength[\"sample_elastic\"].coords[\"spectrum\"].shape[0]} histograms.')\n",
    "            #Fitting the masked sample\n",
    "\n",
    "\n",
    "            sample_fit = sc.compat.mantid.fit(sample_ws,\n",
    "                                          function=f'name=LinearBackground,A0={230},A1={-4};name=UserFunction,Formula=h*erf(a*(x-x0)),h={16},a={-11},x0={xpos_guess}',\n",
    "                                          workspace_index=0, start_x=x_min_fit, end_x=x_max_fit)\n",
    "\n",
    "            params = dict(zip(sample_fit[\"parameters\"].value[\"Name\"].values, sample_fit[\"parameters\"].value[\"Value\"].values))\n",
    "\n",
    "            # take the fitting parameters and pass them on to the forthcoming fits\n",
    "            d_sample = params[\"f1.x0\"] / 2.0 # See fit table definition for extract x0\n",
    "            fit_values = (params[\"f0.A0\"], params[\"f0.A1\"], params[\"f1.h\"], params[\"f1.a\"], params[\"f1.x0\"])\n",
    "\n",
    "\n",
    "        plots_counter = 0 # sets the workspace index to zero then increases one by one\n",
    "        for row in range(y_size):\n",
    "            for col in range(x_size):\n",
    "                ws_index = plots[plots_counter][0]\n",
    "\n",
    "                # if the inital sample was grouped\n",
    "                if \"spectrum\" in rebinned_wavelength[\"sample\"].coords:\n",
    "\n",
    "                    sample_fit = sc.compat.mantid.fit(sample_ws,\n",
    "                                                  function=f\"name=LinearBackground,A0={230},A1={-4};name=UserFunction,Formula=h*erf(a*(x-x0)),h={16},a={-11},x0={xpos_guess}\",\n",
    "                                                  workspace_index=ws_index, start_x=x_min_fit, end_x=x_max_fit)\n",
    "                    \n",
    "                    # take the fitting parameters and pass them on to the forthcoming fits\n",
    "                    params = dict(zip(sample_fit[\"parameters\"].value[\"Name\"].values, sample_fit[\"parameters\"].value[\"Value\"].values))\n",
    "\n",
    "                    # take the fitting parameters and pass them on to the forthcoming fits\n",
    "                    d_sample = params[\"f1.x0\"] # See fit table definition for extract x0\n",
    "                    fit_values = (params[\"f0.A0\"], params[\"f0.A1\"], params[\"f1.h\"], params[\"f1.a\"], params[\"f1.x0\"])\n",
    "\n",
    "                # Fit Bragg edge using values from fit of unstrained sample\n",
    "                elastic_function = f\"name=LinearBackground,A0={fit_values[0]},A1={fit_values[1]};name=UserFunction,Formula=h*erf(a*(x-x0)),h={fit_values[2]},a={fit_values[3]},x0={fit_values[4]}\"\n",
    "                fit_elastic = sc.compat.mantid.fit(elastic_ws,\n",
    "                                                          function=elastic_function,\n",
    "                                                          workspace_index=ws_index, start_x=x_min_fit, end_x=x_max_fit)\n",
    "                elastic_params = dict(zip(fit_elastic[\"parameters\"].value[\"Name\"].values, fit_elastic[\"parameters\"].value[\"Value\"].values))\n",
    "                d_sample_elastic = elastic_params[\"f1.x0\"] / 2.0 # See fit table definition for extract x0\n",
    "                lattice_strain = d_sample_elastic - d_sample\n",
    "\n",
    "                # define successful fitting\n",
    "                success = (sample_fit.attrs[\"status\"].value == \"success\") and (fit_elastic.attrs[\"status\"].value == \"success\")\n",
    "\n",
    "                # fitted values STORED in list\n",
    "                fit_list[c].append((ws_index, (row, col), bragg_edge, d_sample, d_sample_elastic, lattice_strain, success))\n",
    "\n",
    "                # workspace created from sample fit, workspace index 1 for fitted spectrum (index 0 for data, index 2 for difference curve)\n",
    "                fitted_sample = sample_fit[\"workspace\"].data.values[\"empty\", 1]\n",
    "                \n",
    "                if sample_fit.attrs[\"status\"].value == \"success\":\n",
    "                    spectrum_list_fitted_sample[c].append((fitted_sample.coords[\"wavelength\"].values, fitted_sample.values))\n",
    "                else:\n",
    "                    spectrum_list_fitted_sample[c].append((fitted_sample.coords[\"wavelength\"].values,\n",
    "                                                           np.zeros_like(fitted_sample.values)))\n",
    "                    \n",
    "                fitted_elastic = fit_elastic[\"workspace\"].data.values[\"empty\", 1]\n",
    "\n",
    "                # workspace created from fit of sample under elastic deformation\n",
    "                if fit_elastic.attrs[\"status\"].value == \"success\":\n",
    "                    spectrum_list_fitted_sample_elastic[c].append((fitted_elastic.coords[\"wavelength\"].values, fitted_elastic.values))\n",
    "                else:\n",
    "                    spectrum_list_fitted_sample_elastic[c].append((fitted_elastic.coords[\"wavelength\"].values, \n",
    "                                                                                               np.zeros_like(fitted_elastic.values)))\n",
    "                plots_counter+=1\n",
    "\n",
    "    return fit_list, spectrum_list_fitted_sample, spectrum_list_fitted_sample_elastic\n",
    "\n",
    "fit_list, spectrum_list_fitted_sample, spectrum_list_fitted_sample_elastic = \\\n",
    "    Bragg_edge_position(xpos_Bragg_edge=Bragg_edges_FCC,\n",
    "                                      x_min_sides=[0.05, 0.1, 0.1, 0.05],#[0.1, 0.1, 0.1, 0.05]\n",
    "                                      x_max_sides=[0.1, 0.05, 0.1, 0.1],#[0.1, 0.1, 0.1, 0.1]\n",
    "                                      x_size=nx_target,\n",
    "                                      y_size=ny_target, \n",
    "                                      plots=plots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def write_ws_to_ascii(input_list, output_filename_stem, output_directory):\n",
    "    '''\n",
    "    Takes histogram from mantid workspace and writes an ASCII file.\n",
    "    New filename will be the filename of the input workspace and\n",
    "    a given extension appended.\n",
    "\n",
    "    :param input_ws: input workspace from mantid\n",
    "    :param output_directory: Where to write the new file\n",
    "    :param output_extension: which extension should the ASCII file have\n",
    "\n",
    "    :raises: Error if file exists\n",
    "    '''\n",
    "\n",
    "    output_filenames = []\n",
    "    for c, li in enumerate(input_list):\n",
    "        bragg_edge = '({}{}{})'.format(li[0][2][1], li[0][2][2],li[0][2][3])\n",
    "        output_filename = \"_{}_{:.3f}A_{}.txt\".format(output_filename_stem, li[0][3], bragg_edge)\n",
    "        if not isinstance(li, list):\n",
    "            print(\"Input is not a list.\")\n",
    "            return\n",
    "        whole_path = os.path.join(output_directory,output_filename)\n",
    "        if os.path.isfile(whole_path):\n",
    "            sys.stderr.write(\"ERROR: file {:s} already exists!\\n\".format(output_filename))\n",
    "            return\n",
    "        with open(whole_path, 'w') as output_file:\n",
    "            for line in li:\n",
    "                for element in line:\n",
    "                    if not element == line[-1]:\n",
    "                        output_file.write(\"{}\\t\".format(element))\n",
    "                    else:\n",
    "                        output_file.write(\"{}\\n\".format(element))\n",
    "        print(\"File {} was created.\".format(output_filename))\n",
    "        output_filenames.append(output_filename)\n",
    "    return output_filenames\n",
    "\n",
    "tbin_width = 64*2.5\n",
    "output_filename_table_stem = '{:03d}_table_strain_analysis_{}xy_{}usbin_{}thresh_initintegr{}'.format(\n",
    "                        measurement_number,grouping_number,tbin_width,masking_threshold, str(integrating_of_sample_in_inital_state))\n",
    "\n",
    "\n",
    "\n",
    "write_ws_to_ascii(fit_list, output_filename_table_stem, output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def tileplot_colorcode(fit_list, nx_target, ny_target, outlier_threshold, output_filename_stem='tileplot_color'):\n",
    "    # for each part of the fit list containing the fit values of each Bragg edge, create a color plot.\n",
    "    output_filename_list = []\n",
    "    for fit_be_list in fit_list:\n",
    "        bragg_edge = '({}{}{})'.format(fit_be_list[0][2][1],fit_be_list[0][2][2],fit_be_list[0][2][3])  # the calculated Bragg edge TOF position\n",
    "        d_spacing = fit_be_list[0][3] # the calculated Bragg edge TOF position\n",
    "        output_filename = '{}_{:.3f}A_{}'.format(output_filename_stem, d_spacing, bragg_edge)\n",
    "        # Make a 2D image of the strain values\n",
    "        print(('Plotting color-coded tile plot of Bragg edge {}.'.format(bragg_edge)))\n",
    "        fig, ax = plt.subplots()\n",
    "        strains = np.zeros([ny_target, nx_target])\n",
    "        plots_counter = 0\n",
    "        for row in range(ny_target):\n",
    "            for col in range(nx_target):\n",
    "                if (fit_be_list[plots_counter][-1] and outlier_threshold[0] < fit_be_list[plots_counter][-2] < outlier_threshold[1]):\n",
    "                    strains[row, col] = fit_be_list[plots_counter][-2]\n",
    "                plots_counter += 1\n",
    "        import matplotlib.colors as colors\n",
    "        im = ax.imshow(strains, origin=\"upper\", norm=colors.SymLogNorm(linthresh=1.0e-3), cmap=\"RdBu\")\n",
    "        cb = plt.colorbar(im)\n",
    "        cb.set_label(\"${}$ Lattice strain $\\\\varepsilon$\".format(bragg_edge))\n",
    "        print(f'Saving {output_filename}.pdf and .png.')\n",
    "        fig.savefig(\"{}.pdf\".format(output_filename), bbox_inches=\"tight\")\n",
    "        fig.savefig(\"{}.png\".format(output_filename), bbox_inches=\"tight\")\n",
    "        output_filename_list.append(output_filename)\n",
    "    return output_filename_list\n",
    "\n",
    "\n",
    "\n",
    "# i.e. -1e-2, 1e-2 means only -1% to 1% values of lattice strain are shown\n",
    "outlier_threshold_color_plot = (-5e-2, 5e-2)\n",
    "\n",
    "output_filename_tileplot_color ='{:03d}_tileplot_color_{}xy_{}usbin_{}thresh_{:.3f}-{:.3f}plotthresh_initintegr{}'.format(\n",
    "                        measurement_number,grouping_number,tbin_width,masking_threshold,\n",
    "                        outlier_threshold_color_plot[0], outlier_threshold_color_plot[1],\n",
    "                        str(integrating_of_sample_in_inital_state))\n",
    "\n",
    "fignames_col = tileplot_colorcode(fit_list=fit_list, nx_target=nx_target, ny_target=ny_target,\n",
    "                                  outlier_threshold=outlier_threshold_color_plot,\n",
    "                                  output_filename_stem=os.path.join(output_dir, output_filename_tileplot_color))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def edges_to_centers(x):\n",
    "    \"\"\"\n",
    "    Convert array edges to centers\n",
    "    \"\"\"\n",
    "    return 0.5 * (x[1:] + x[:-1])\n",
    "\n",
    "def tileplot(x_min_plot,x_max_plot,y_min_plot,y_max_plot,x_size,y_size,\n",
    "             histogram_matrices,fitted_data, plots, fit_list, output_filename='tileplot_curves'):\n",
    "\n",
    "    fig, ax = plt.subplots(x_size, y_size, figsize=(12, 12))\n",
    "\n",
    "    labels = [\"Without load\", \"Elastic deformation\"]\n",
    "    markers = [\"o\", \"s\"]\n",
    "    # Plot some fake data so that we can have larger markers in the legend\n",
    "    ax[0][0].plot([0, 1], [0, 1], ls=\"None\", marker=markers[0], markersize=4, label=labels[0], color=\"C0\")\n",
    "    ax[0][0].plot([0, 1], [0, 1], ls=\"None\", marker=markers[1], markersize=4, label=labels[1], color=\"C1\")\n",
    "\n",
    "    # All bins are identical in a dataset\n",
    "    x0_nostrain = edges_to_centers(histogram_matrices[0].coords[\"wavelength\"].values)\n",
    "    \n",
    "    # if the full inital sample was taken and no grouping was done\n",
    "    if histogram_matrices[0].coords[\"spectrum\"].shape[0] == 1:\n",
    "        # x and y values of inital state, samples without strain\n",
    "        y0_nostrain = histogram_matrices[0][\"spectrum\", 0].values\n",
    "\n",
    "    plots_counter = 0\n",
    "    for row in range(y_size):\n",
    "        for col in range(x_size):\n",
    "            # for running workspace index\n",
    "            ws_index = plots[plots_counter][0]\n",
    "\n",
    "            # plot if the spectrum is not masked\n",
    "            if not histogram_matrices[1].masks[\"sample_elastic\"][\"spectrum\", ws_index]:\n",
    "\n",
    "                # if the whole inital-sample-state is just one spectrum, always plot that.\n",
    "                if histogram_matrices[0].coords[\"spectrum\"].shape[0] == 1:\n",
    "                    # Plot no-strain curve, inital state of sample\n",
    "                    ax[row][col].plot(x0_nostrain, y0_nostrain, ls=\"None\", marker= \"o\", markersize=1)\n",
    "                else: # else plot each of the grouped spectra accordingly\n",
    "                    y0_nostrain = histogram_matrices[0].extractY()[ws_index]\n",
    "                    ax[row][col].plot(x0_nostrain, y0_nostrain, ls=\"None\", marker= \"o\", markersize=1)\n",
    "\n",
    "                # Plot curves of other states given in histogram_matrices\n",
    "                for k in range(len(histogram_matrices)):\n",
    "                    if k != 0:\n",
    "                        x0 = edges_to_centers(histogram_matrices[k].extractX()[ws_index])\n",
    "                        y0 = histogram_matrices[k].extractY()[ws_index]\n",
    "                        ax[row][col].plot(x0, y0, ls=\"None\", marker=markers[k], markersize=1)\n",
    "\n",
    "                # plotting the fitted spectra:\n",
    "                x1 = edges_to_centers(fitted_data[0][plots_counter][1])\n",
    "                x2 = edges_to_centers(fitted_data[1][plots_counter][1])\n",
    "                ax[row][col].plot(x1, fitted_data[0][plots_counter][2], label=\"{} fit\".format(labels[0]), lw=1, zorder=3)\n",
    "                ax[row][col].plot(x2, fitted_data[1][plots_counter][2], label=\"{} fit\".format(labels[1]), lw=1, zorder=3)\n",
    "\n",
    "                # Set axis limits\n",
    "                ax[row][col].set_xlim([x_min_plot, x_max_plot])\n",
    "                ax[row][col].set_ylim([y_min_plot, y_max_plot])\n",
    "                if col == x_size - 1:\n",
    "                    ax[row][col].yaxis.tick_right()\n",
    "                else:\n",
    "                    ax[row][col].set_yticklabels([])\n",
    "                if row == 0:\n",
    "                    ax[row][col].xaxis.tick_top()\n",
    "                else:\n",
    "                    ax[row][col].set_xticklabels([])\n",
    "                ax[row][col].tick_params(axis=\"x\", direction=\"in\", bottom=True, top=True)\n",
    "                ax[row][col].tick_params(axis=\"y\", direction=\"in\", left=True, right=True)\n",
    "\n",
    "                if row == y_size - 1:\n",
    "                    ax[row][col].text(0.0, -0.1, \"{:.1f}\".format(0.1 * col),\n",
    "                                    ha='center',va='top', transform=ax[row][col].transAxes)\n",
    "                if col == 0:\n",
    "                    ax[row][col].text(-0.1, 0.0, \"{:.1f}\".format(0.1*(y_size - row - 1)),\n",
    "                                    ha='right',va='center', transform=ax[row][col].transAxes)\n",
    "\n",
    "                if fit_list[plots_counter][-1]: # usual sign for lattice strain is \\\\varepsilon\n",
    "                    ax[row][col].text(0.1, 0.8, \"${:.3f}$\".format(fit_list[plots_counter][-2]),\n",
    "                                    ha='left',va='top', transform=ax[row][col].transAxes, fontsize=4)\n",
    "            plots_counter += 1\n",
    "\n",
    "    # Last bin edge for xy coordinates\n",
    "    ax[-1][-1].text(1.0, -0.1, \"{:.1f}\".format(0.1 * x_size),\n",
    "                    ha='center',va='top', transform=ax[-1][-1].transAxes)\n",
    "    ax[0][0].text(-0.1, 1.0, \"{:.1f}\".format(0.1 * y_size),\n",
    "                  ha='right',va='center', transform=ax[0][0].transAxes)\n",
    "    # Display only one legend\n",
    "    ax[0][0].legend(loc=(0, 1.8), ncol=4)\n",
    "    fig.text(0.5, 0.905, \"Wavelength $[\\mathrm{\\AA}]$\")\n",
    "    fig.text(0.94, 0.5, \"Counts\", rotation=90, ha='center',va='center')\n",
    "    fig.text(0.5, 0.075, \"X position [m]\")\n",
    "    fig.text(0.09, 0.5, \"Y position [m]\", rotation=90, ha='center',va='center')\n",
    "    # Remove white space between subplots\n",
    "    fig.subplots_adjust(wspace=0.0, hspace=0.0)\n",
    "    print(('Saving {}.pdf.').format(output_filename))\n",
    "    fig.savefig(\"{}.pdf\".format(output_filename), bbox_inches=\"tight\")\n",
    "    fig.savefig(\"{}.png\".format(output_filename), bbox_inches=\"tight\")\n",
    "    return output_filename\n",
    "\n",
    "output_filename_tileplot = '{:03d}_tileplot_curves_{}xy_{}usbin_{}thresh_initintegr{}'.format(\n",
    "                        measurement_number,grouping_number,tbin_width,masking_threshold, str(integrating_of_sample_in_inital_state))\n",
    "\n",
    "# Plotting: y_min_plot=200,y_max_plot=270 for 18x18, y_min_plot=90,y_max_plot=120 for 27x27.\n",
    "plotting_win = [(i[1]*(1-.135), i[1]*(1+.135)) for i in Bragg_edges_FCC]\n",
    "for i in range(len(Bragg_edges_FCC)):\n",
    "    output_filename_tileplot_new = '{}_{:.3f}A_({}{}{})'.format(output_filename_tileplot, Bragg_edges_FCC[i][1]/2.,\n",
    "                                                        Bragg_edges_FCC[i][0][0], Bragg_edges_FCC[i][0][1], Bragg_edges_FCC[i][0][2])\n",
    "    win = plotting_win[i]\n",
    "    fignames_curve = tileplot(x_min_plot=win[0],x_max_plot=win[1],y_min_plot=65610./(nx_target*ny_target),y_max_plot=87480./(nx_target*ny_target), # remove 0.9 later\n",
    "        x_size=nx_target,y_size=ny_target,\n",
    "        histogram_matrices=histogram_matrices,\n",
    "        fitted_data=[spectrum_list_fitted_sample[i], spectrum_list_fitted_sample_elastic[i]],\n",
    "        plots=plots, fit_list=fit_list[i], output_filename=os.path.join(output_dir, output_filename_tileplot_new))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}